import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig

# --- 1. AYARLAR ---
model_name = "/kaggle/input/mathmodellarge/pytorch/default/1/final_unwrapped"
new_model_name = "uhem-dpo-model"

# --- 2. MODEL VE TOKENIZER ---
model = AutoModelForCausalLM.from_pretrained(
Â  Â  model_name,
Â  Â  torch_dtype=torch.float16,
Â  Â  device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_tokenÂ 
tokenizer.padding_side = "left" # DPO iÃ§in soldan padding Ã¶nemlidir

# --- 3. VERÄ° SETÄ° ---
dataset = load_dataset("json", data_files="dpo_data.json", split="train")

# --- 4. LORA AYARLARI ---
peft_config = LoraConfig(
Â  Â  r=32,
Â  Â  lora_alpha=64,
Â  Â  lora_dropout=0.05,
Â  Â  bias="none",
Â  Â  task_type="CAUSAL_LM",
Â  Â  target_modules=["c_attn", "c_proj", "c_fc"] # Model mimarisine gÃ¶re burasÄ± deÄŸiÅŸebilir
)

# --- 5. EÄžÄ°TÄ°M KONFÄ°GÃœRASYONU ---
training_args = DPOConfig(
Â  Â  output_dir="./dpo_results",
Â  Â  beta=0.1,
Â  Â Â 
Â  Â  # --- Performans AyarlarÄ± ---
Â  Â  learning_rate=5e-6, Â  Â  Â  Â  Â  Â  Â # KonuÅŸtuÄŸumuz deÄŸer (SFT'nin Ã§eyreÄŸi)
Â  Â  num_train_epochs=1, Â  Â  Â  Â  Â  Â  Â # Tek tur yeterli
Â  Â  per_device_train_batch_size=2, 
Â  Â  gradient_accumulation_steps=8, Â # 2x8 = 16 Batch Size etkisi (KararlÄ±lÄ±k iÃ§in)
Â  Â Â 
Â  Â  # --- IsÄ±nma (Warm-up) AyarlarÄ± ---
Â  Â  # SENÄ°N SORDUÄžUN VE EKSÄ°K OLAN KISIM BURASI:
Â  Â  warmup_ratio=0.1, Â  Â  Â  Â  Â  Â  Â  Â # EÄŸitimin ilk %10'unda yavaÅŸ baÅŸla, aÄŸÄ±rlÄ±k gÃ¼ncellemeyi direkt lr Ã¼zerinden yaparak bÃ¼yÃ¼k deÄŸiÅŸiklik yapmayÄ± engeller.
Â  Â  lr_scheduler_type="cosine", Â  Â  Â # Sonlara doÄŸru yavaÅŸÃ§a dur
Â  Â Â 
Â  Â  logging_steps=10,
Â  Â  save_steps=100,
Â  Â  fp16=True,
Â  Â  optim="paged_adamw_32bit", Â  Â  Â  # RAM optimizasyonu
Â  Â  remove_unused_columns=False
)

# --- 6. TRAINER BAÅžLATMA ---
trainer = DPOTrainer(
Â  Â  model=model,
Â  Â  ref_model=None,
Â  Â  args=training_args,
Â  Â  train_dataset=dataset,
Â  Â  processing_class=tokenizer, # Yeni sÃ¼rÃ¼m iÃ§in dÃ¼zeltme
Â  Â  peft_config=peft_config,
Â  Â  max_prompt_length=512,
Â  Â  max_length=1024,
)

# --- 7. BAÅžLAT ---
print("ðŸš€ DPO EÄŸitimi (Warm-up ile) BaÅŸlÄ±yor...")
trainer.train()

# --- 8. KAYDET ---
trainer.model.save_pretrained(new_model_name)
tokenizer.save_pretrained(new_model_name)
print(f"âœ… Model {new_model_name} klasÃ¶rÃ¼ne kaydedildi!")
