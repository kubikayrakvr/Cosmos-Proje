import torch
from datasets import load_dataset
import kagglehub
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, PeftModel

model_name = "/kaggle/input/mathmodellarge/pytorch/default/1/final_unwrapped"  #kaggle'dan Ã§ekilmiÅŸ pre-train edilmiÅŸ model
new_model_name = "uhem-dpo-model"

# T4 GPU iÃ§in float16, daha yeni kartlar iÃ§in bfloat16 kullanÄ±labilir
dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# --- 2. MODEL VE TOKENIZER ---
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=dtype,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token 
tokenizer.padding_side = "left" # DPO iÃ§in sol padding ÅART

print("Model baÅŸarÄ±yla yÃ¼klendi! Test edebilirsin.")

dataset_file = "/kaggle/input/full-dataset-csv/full_dataset.csv"  # Senin yÃ¼klediÄŸin dosya

# ==========================================
# 2. MODEL VE TOKENIZER HAZIRLIÄI
# ==========================================
print("ğŸ“¥ Model ve Tokenizer yÃ¼kleniyor...")

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" # DPO iÃ§in KRÄ°TÄ°K AYAR: Sol padding

# ==========================================
# 3. VERÄ° SETÄ° HAZIRLIÄI VE KONTROLÃœ
# ==========================================
print(f"ğŸ“‚ '{dataset_file}' dosyasÄ± yÃ¼kleniyor...")
dataset = load_dataset("csv", data_files=dataset_file, split="train")

# Veri setini %90 EÄŸitim, %10 Test olarak ikiye bÃ¶lÃ¼yoruz
# Bu sayede modelin "ezberleyip ezberlemediÄŸini" anlayacaÄŸÄ±z.
dataset_split = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

print(f"ğŸ“Š EÄŸitim Verisi: {len(train_dataset)} satÄ±r")
print(f"ğŸ“Š Test Verisi:   {len(eval_dataset)} satÄ±r")

# Formatlama Fonksiyonu
def format_dpo_data(example):
    # SFT formatÄ±na uygun hale getiriyoruz
    # Soru: ### Question: ...
    # Cevap: ### Answer: (BurayÄ± model tamamlayacak)
    
    return {
        "prompt": f"### Question:\n{example['prompt']}\n\n### Answer:\n",
        "chosen": example['chosen'],   # Ä°yi cevap
        "rejected": example['rejected'] # KÃ¶tÃ¼ cevap
    }

print("âš™ï¸ Veri seti DPO formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
train_dataset = train_dataset.map(format_dpo_data)
eval_dataset = eval_dataset.map(format_dpo_data)

# ==========================================
# 4. LORA (AKILLI ADAPTÃ–R) AYARLARI
# ==========================================
peft_config = LoraConfig(
    r=32,
    lora_alpha=64,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_attn", "c_proj", "c_fc"] # GPT-2 katmanlarÄ±
)

# ==========================================
# 5. EÄÄ°TÄ°M AYARLARI (GELÄ°ÅMÄ°Å RAPORLAMA)
# ==========================================
# Otomatik Warmup HesabÄ±: Verinin %5'i kadar Ä±sÄ±nma
total_steps = len(train_dataset) // 2  # Batch size 2 olduÄŸu iÃ§in
warmup_steps = int(total_steps * 0.05) 

training_args = DPOConfig(
    output_dir="./dpo_results",
    beta=0.1,                    # DPO'nun deÄŸiÅŸim katsayÄ±sÄ± (Standart 0.1)
    learning_rate=5e-6,          # Ã‡ok hassas, yavaÅŸ Ã¶ÄŸrenme hÄ±zÄ±
    num_train_epochs=1,          # Tek tur yeterli
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8, 
    
    # --- Raporlama ve Takip AyarlarÄ± ---
    eval_strategy="steps",       # Belirli adÄ±mlarda test yap
    eval_steps=50,               # Her 50 adÄ±mda bir karnesini gÃ¶r
    save_steps=100,              # Her 100 adÄ±mda bir kaydet
    logging_steps=10,            # Her 10 adÄ±mda bir ekrana bilgi bas
    
    warmup_steps=warmup_steps,   # Dinamik hesapladÄ±ÄŸÄ±mÄ±z Ä±sÄ±nma adÄ±mÄ±
    lr_scheduler_type="cosine",  # Sonlara doÄŸru yavaÅŸlayan akÄ±llÄ± tarife
    
    fp16=True,                   # T4 GPU uyumu
    optim="paged_adamw_32bit",   # RAM tasarrufu saÄŸlayan optimizer
    remove_unused_columns=False,

    max_prompt_length=512,
    max_length=1024,
)

# ==========================================
# 6. EÄÄ°TÄ°MÄ° BAÅLATMA
# ==========================================
trainer = DPOTrainer(
    model=model,
    ref_model=None, # None yapÄ±nca orjinal modeli referans alÄ±r (HafÄ±za tasarrufu)
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,   # Test verisini buraya verdik
    processing_class=tokenizer,
    peft_config=peft_config,
)

print("\nğŸš€ DPO EÄŸitimi BaÅŸlÄ±yor! (Ã‡Ä±ktÄ±larÄ± Takip Et)")
print("Ekranda 'rewards/margins' artÄ±yorsa model akÄ±llanÄ±yor demektir.")
print("-" * 50)

trainer.train()

# ==========================================
# 7. KAYDETME VE FÄ°NAL TESTÄ°
# ==========================================
print("\nğŸ’¾ Model kaydediliyor...")
trainer.model.save_pretrained(new_model_name)
tokenizer.save_pretrained(new_model_name)

print("ğŸ‰ EÄŸitim TamamlandÄ±! Åimdi ufak bir test yapalÄ±m...")

# Basit bir Inference (Ã‡Ä±karÄ±m) Testi
def generate_test(prompt_text):
    inputs = tokenizer(f"### Question:\n{prompt_text}\n\n### Answer:\n", return_tensors="pt").to(model.device)
    # Model cevap Ã¼retirken Ã¶nceki ayarlarÄ±nÄ± kullansÄ±n
    outputs = model.generate(
        **inputs, 
        max_new_tokens=100, 
        do_sample=True, 
        temperature=0.7
    )
    print(f"\nâ“ Soru: {prompt_text}")
    print(f"ğŸ’¡ Cevap:\n{tokenizer.decode(outputs[0], skip_special_tokens=True)}")

# Datasetten rastgele bir soruyu test et
sample_prompt = eval_dataset[0]['prompt'].replace("### Question:\n", "").replace("\n\n### Answer:\n", "")
generate_test(sample_prompt)
