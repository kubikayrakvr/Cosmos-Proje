import torch
import os
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model

# ==========================================
# 1. AYARLAR VE YOLLAR
# ==========================================
# Kaggle'daki model ve veri seti yollarÄ±
MODEL_PATH = "/kaggle/working/merged_model"   
DATASET_PATH = "/kaggle/working/dpo_dataset_final.jsonl"
NEW_MODEL_NAME = "dpo_final_model"

# GPU tipine gÃ¶re veri tipi seÃ§imi (T4 -> float16)
dtype = torch.float16 if torch.cuda.is_available() else torch.float32

print(f"ğŸ“¥ Model yÃ¼kleniyor: {MODEL_PATH}")

# ==========================================
# 2. MODEL VE TOKENIZER
# ==========================================
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=dtype,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# DPO iÃ§in padding ayarlarÄ± KRÄ°TÄ°KTÄ°R
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"  # Decoder modellerde DPO iÃ§in SOL padding ÅŸarttÄ±r

print("âœ… Model ve Tokenizer hazÄ±r.")

# ==========================================
# 3. VERÄ° SETÄ° YÃœKLEME VE FORMATLAMA (GÃœNCELLENDÄ°)
# ==========================================
print(f"ğŸ“‚ Veri seti yÃ¼kleniyor: {DATASET_PATH}")

# JSONL formatÄ±nda olduÄŸu iÃ§in "json" yÃ¼kleyicisi kullanÄ±yoruz
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

# --- YENÄ° EKLENEN KISIM BAÅLANGIÃ‡ ---
def format_prompt(example):
    # Prompt'un baÅŸÄ±na '### Soru: ' ve sonuna '\n### Cevap:' ekle
    # strip() ile olasÄ± fazladan boÅŸluklarÄ± temizleyip formatÄ± oturtuyoruz
    example["prompt"] = f"### Soru: {example['prompt'].strip()}\n### Cevap:"
    return example

# Veri setini formatla
print("ğŸ› ï¸ Promptlar formatlanÄ±yor (### Soru/Cevap ekleniyor)...")
dataset = dataset.map(format_prompt)
# --- YENÄ° EKLENEN KISIM BÄ°TÄ°Å ---

# Veriyi %90 EÄŸitim, %10 Test olarak ayÄ±rÄ±yoruz
dataset_split = dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

print(f"ğŸ“Š EÄŸitim Verisi: {len(train_dataset)} satÄ±r")
print(f"ğŸ“Š Test Verisi:   {len(eval_dataset)} satÄ±r")
print(f"ğŸ” Ã–rnek Prompt: {train_dataset[0]['prompt']}") # Kontrol iÃ§in yazdÄ±r

# ==========================================
# 4. LORA (PEFT) AYARLARI
# ==========================================
# Modeli komple eÄŸitmek yerine, sadece adaptÃ¶rleri eÄŸiteceÄŸiz (HafÄ±za tasarrufu)
peft_config = LoraConfig(
    r=16,                # AdaptÃ¶r boyutu
    lora_alpha=32,       # Ã–lÃ§eklendirme
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_attn", "c_proj", "c_fc"] # GPT-2 katmanlarÄ±
)

# ==========================================
# 5. EÄÄ°TÄ°M AYARLARI (DPO CONFIG)
# ==========================================
training_args = DPOConfig(
    output_dir="./dpo_results",
    
    # --- DPO Parametreleri ---
    beta=0.1,                   # DPO'nun 'cezalandÄ±rma' katsayÄ±sÄ± (Genelde 0.1)
    loss_type="sigmoid",        # Standart DPO kaybÄ±
    
    # --- EÄŸitim HÄ±zÄ± ve SÃ¼resi ---
    learning_rate=5e-6,         # DPO Ã§ok hassastÄ±r, dÃ¼ÅŸÃ¼k LR kullanÄ±yoruz
    num_train_epochs=1,         # 1 Epoch genellikle yeterlidir
    per_device_train_batch_size=2, # GPU hafÄ±zasÄ±na gÃ¶re (T4 iÃ§in 2)
    gradient_accumulation_steps=8, # Sanal batch size (2*8 = 16 gibi davranÄ±r)
    
    # --- Raporlama ---
    eval_strategy="steps",      # AdÄ±m bazlÄ± test
    eval_steps=50,              # Her 50 adÄ±mda bir test et
    save_steps=100,             # Her 100 adÄ±mda bir kaydet
    logging_steps=10,           # Her 10 adÄ±mda bir log bas
    
    # --- Optimizasyon ---
    warmup_ratio=0.1,           # EÄŸitimin %10'u kadar Ä±sÄ±nma
    lr_scheduler_type="cosine", # YavaÅŸlayan Ã¶ÄŸrenme hÄ±zÄ±
    fp16=True,                  # HÄ±z ve hafÄ±za iÃ§in
    optim="paged_adamw_32bit",  # RAM dostu optimizer
    
    # --- Uzunluk AyarlarÄ± ---
    max_prompt_length=512,      # Sorunun uzunluÄŸu
    max_length=1024,            # Toplam uzunluk (Soru + Cevap)
    remove_unused_columns=False 
)

# ==========================================
# 6. EÄÄ°TÄ°MÄ° BAÅLATMA
# ==========================================
print("\nğŸš€ DPO EÄŸitimi BaÅŸlÄ±yor...")
print("â„¹ï¸  Takip Etmen Gerekenler:")
print("   - rewards/chosen (ArtmalÄ±): Modelin iyi cevaba verdiÄŸi puan")
print("   - rewards/rejected (AzalmalÄ±): Modelin kÃ¶tÃ¼ cevaba verdiÄŸi puan")
print("   - rewards/margins (ArtmalÄ±): Ä°yi ile kÃ¶tÃ¼ arasÄ±ndaki fark")
print("-" * 50)

trainer = DPOTrainer(
    model=model,
    ref_model=None, # None = Mevcut modeli kopyalayÄ±p dondurur (Otomatik referans)
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    processing_class=tokenizer, 
    peft_config=peft_config,
)

trainer.train()

# ==========================================
# 7. KAYDETME
# ==========================================
print(f"\nğŸ’¾ Model kaydediliyor: {NEW_MODEL_NAME}")
trainer.model.save_pretrained(NEW_MODEL_NAME)
tokenizer.save_pretrained(NEW_MODEL_NAME)

print("ğŸ‰ Tebrikler! DPO EÄŸitimi TamamlandÄ±.")

# ==========================================
# 8. HIZLI TEST
# ==========================================
def test_model(prompt_text):
    # Prompt formatÄ±nÄ± veri setindeki gibi yapÄ±yoruz
    # Ancak dataset iÃ§indeki prompt zaten formatlÄ± olduÄŸu iÃ§in burada sadece
    # temiz halini alÄ±p tekrar formatlayacaÄŸÄ±z veya direkt vereceÄŸiz.
    # GÃ¼venlik iÃ§in manuel formatlayalÄ±m:
    full_prompt = f"### Soru: {prompt_text}\n### Cevap:"
    
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=100, 
            do_sample=True, 
            temperature=0.7
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test verisinden rastgele bir Ã¶rnek alÄ±p deneyelim
print("\nğŸ§ª Test YapÄ±lÄ±yor...")
try:
    # Prompt sÃ¼tunundan ham soruyu Ã§ekip alalÄ±m (FormatÄ± temizleyerek)
    # NOT: ArtÄ±k prompt iÃ§inde ### Soru var, o yÃ¼zden replace ile temizleyip test ediyoruz
    raw_prompt = eval_dataset[0]['prompt'].replace("### Soru: ", "").replace("\n### Cevap:", "").strip()
    
    print(f"Soru: {raw_prompt}")
    print("-" * 20)
    print(f"Modelin CevabÄ±:\n{test_model(raw_prompt)}")
except Exception as e:
    print(f"Test sÄ±rasÄ±nda hata oluÅŸtu: {e}")
