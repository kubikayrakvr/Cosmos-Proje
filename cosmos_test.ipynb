{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:35:15.401297600Z",
     "start_time": "2026-02-06T14:35:09.732967500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (80.10.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yusuf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\Yusuf\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\Yusuf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan cihaz: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers accelerate tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cihaz kontrolü (GPU varsa GPU, yoksa CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Kullanılan cihaz: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40f68895d20b621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:35:30.651625100Z",
     "start_time": "2026-02-06T14:35:21.430598900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 292/292 [00:00<00:00, 435.20it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    }
   ],
   "source": [
    "# --- BURAYI GÜNCELLE ---\n",
    "model_yolu = r\"C:\\Users\\Yusuf\\.cache\\kagglehub\\models\\kubikay\\mathmodellarge\\pyTorch\\default\\1\\final_unwrapped\"# -----------------------\n",
    "\n",
    "print(\"Model yükleniyor...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_yolu)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_yolu,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# Eğer tokenizer'ın pad_token'ı yoksa (Llama gibi modellerde gerekebilir)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71866e4ef4d361ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, tokenizer, test_texts, batch_size=4):\n",
    "    model.eval()\n",
    "    total_top1 = 0\n",
    "    total_top5 = 0\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    # Verileri batch'lere bölüyoruz\n",
    "    for i in tqdm(range(0, len(test_texts), batch_size), desc=\"Değerlendiriliyor\"):\n",
    "        batch_texts = test_texts[i : i + batch_size]\n",
    "\n",
    "        # Tokenization\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Kaydırma (Shift): Modelin N. tahmini N+1. token ile kıyaslanır\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            # Padding olan yerleri (ignore_index = -100 veya pad_id) hesaplamadan çıkarıyoruz\n",
    "            mask = shift_labels != tokenizer.pad_token_id\n",
    "\n",
    "            # Top-k hesaplama\n",
    "            _, top5_indices = shift_logits.topk(5, dim=-1)\n",
    "\n",
    "            # Doğru tahminleri say (Sadece maskelenmemiş yani gerçek tokenlar için)\n",
    "            top1_correct = ((top5_indices[:, :, 0] == shift_labels) & mask).sum().item()\n",
    "            top5_correct = ((top5_indices == shift_labels.unsqueeze(-1)).any(dim=-1) & mask).sum().item()\n",
    "\n",
    "            total_top1 += top1_correct\n",
    "            total_top5 += top5_correct\n",
    "            total_tokens += mask.sum().item()\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "\n",
    "    avg_top1 = (total_top1 / total_tokens) * 100\n",
    "    avg_top5 = (total_top5 / total_tokens) * 100\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "\n",
    "    return {\n",
    "        \"Top-1 Accuracy (%)\": round(avg_top1, 2),\n",
    "        \"Top-5 Accuracy (%)\": round(avg_top5, 2),\n",
    "        \"Perplexity\": round(perplexity, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8404de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sft_performance(model, tokenizer, test_samples, batch_size=2):\n",
    "    model.eval()\n",
    "    total_top1, total_top5, total_tokens, total_loss = 0, 0, 0, 0\n",
    "    \n",
    "    # SFT modellerinde genellikle cevap bir ayırıcıdan sonra gelir\n",
    "    # Kullandığın modele göre \"Cevap:\" veya \"\\n### Response:\" gibi güncelleyebilirsin\n",
    "    response_delimiter = \"Cevap:\" \n",
    "\n",
    "    for i in tqdm(range(0, len(test_samples), batch_size)):\n",
    "        batch_texts = test_samples[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # --- SFT MASKELME ---\n",
    "        # Soru kısmındaki tokenları başarı hesabına dahil etmemek için -100 ile maskeliyoruz\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            if response_delimiter in text:\n",
    "                # Soru ve cevap kısmını ayırıyoruz\n",
    "                prompt_part = text.split(response_delimiter)[0] + response_delimiter\n",
    "                prompt_token_len = len(tokenizer.encode(prompt_part, add_special_tokens=False))\n",
    "                # Soru kısmını -100 yap (loss ve doğruluk hesabında görmezden gelinir)\n",
    "                labels[j, :prompt_token_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "            # Sadece -100 olmayan (yani cevap olan) yerleri al\n",
    "            mask = shift_labels != -100\n",
    "            \n",
    "            if mask.sum() == 0: continue # Boş batch kontrolü\n",
    "\n",
    "            _, top5_indices = shift_logits.topk(5, dim=-1)\n",
    "            \n",
    "            top1_correct = ((top5_indices[:, :, 0] == shift_labels) & mask).sum().item()\n",
    "            top5_correct = ((top5_indices == shift_labels.unsqueeze(-1)).any(dim=-1) & mask).sum().item()\n",
    "            \n",
    "            total_top1 += top1_correct\n",
    "            total_top5 += top5_correct\n",
    "            total_tokens += mask.sum().item()\n",
    "            total_loss += outputs.loss.item() * mask.sum().item()\n",
    "\n",
    "    return {\n",
    "        \"SFT Top-1 Acc (%)\": round((total_top1 / total_tokens) * 100, 2),\n",
    "        \"SFT Top-5 Acc (%)\": round((total_top5 / total_tokens) * 100, 2),\n",
    "        \"SFT Perplexity\": round(torch.exp(torch.tensor(total_loss / total_tokens)).item(), 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6257ba69907bf2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K-TR (malhajar) veri seti yükleniyor...\n",
      "Toplam 1318 örnek hazırlandı. (test spliti kullanılıyor)\n",
      "\n",
      "--- GSM8K-TR Üzerinde Değerlendirme Başlatılıyor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Değerlendiriliyor:   0%|          | 0/659 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Değerlendiriliyor: 100%|██████████| 659/659 [00:31<00:00, 21.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GSM8K-TR EVAL SONUÇLARI ---\n",
      "Top-1 Accuracy (%): 45.55\n",
      "Top-5 Accuracy (%): 67.07\n",
      "Perplexity: 15.9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Veri setini 'main' config'i ile yüklüyoruz\n",
    "print(\"GSM8K-TR (malhajar) veri seti yükleniyor...\")\n",
    "dataset = load_dataset(\"malhajar/gsm8k-tr\", \"main\")\n",
    "\n",
    "# 2. Test verisini hazırla\n",
    "test_samples = []\n",
    "\n",
    "# Bu veri setinde split isimleri genelde 'train' ve 'test'tir.\n",
    "# Kontrol amaçlı split ismini belirliyoruz:\n",
    "split_name = \"test\" if \"test\" in dataset else \"train\"\n",
    "\n",
    "for example in dataset[split_name]:\n",
    "    # Sütun isimleri bu veri setinde 'question' ve 'answer'dır.\n",
    "    full_text = f\"Soru: {example['question']}\\nCevap: {example['answer']}\"\n",
    "    test_samples.append(full_text)\n",
    "\n",
    "print(f\"Toplam {len(test_samples)} örnek hazırlandı. ({split_name} spliti kullanılıyor)\")\n",
    "\n",
    "# 3. Değerlendirmeyi Çalıştır (Önceki hücredeki fonksiyonu kullanır)\n",
    "print(\"\\n--- GSM8K-TR Üzerinde Değerlendirme Başlatılıyor ---\")\n",
    "results = evaluate_model_performance(model, tokenizer, test_samples, batch_size=2)\n",
    "\n",
    "print(\"\\n--- GSM8K-TR EVAL SONUÇLARI ---\")\n",
    "for metrik, deger in results.items():\n",
    "    print(f\"{metrik}: {deger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32505413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model ve Tokenizer yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 292/292 [00:00<00:00, 403.10it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA ağırlıkları (PEFT) giydiriliyor...\n",
      "LoRA modeli başarıyla birleştirildi ve hazır!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# --- AYARLAR ---\n",
    "base_model_yolu = r\"C:\\Users\\Yusuf\\.cache\\kagglehub\\models\\kubikay\\mathmodellarge\\pyTorch\\default\\1\\final_unwrapped\"# -----------------------\n",
    "lora_weights_yolu = r\"C:\\Users\\Yusuf\\.cache\\kagglehub\\models\\kubikay\\sft-lora\\pyTorch\\default\\1\\final_model\"\n",
    "# ---------------\n",
    "\n",
    "print(\"Base model ve Tokenizer yükleniyor...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_yolu)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_yolu,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"LoRA ağırlıkları (PEFT) giydiriliyor...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_weights_yolu)\n",
    "model = model.to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"LoRA modeli başarıyla birleştirildi ve hazır!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aae3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LoRA Modeli GSM8K-TR Üzerinde Test Ediliyor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [00:58<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANALİZ SONUÇLARI ---\n",
      "SFT Top-1 Acc (%): 70.36\n",
      "SFT Top-5 Acc (%): 85.13\n",
      "SFT Perplexity: 4.3515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Veriyi çek\n",
    "dataset = load_dataset(\"malhajar/gsm8k-tr\", \"main\")\n",
    "test_samples = [f\"Soru: {ex['question']}\\nCevap: {ex['answer']}\" for ex in dataset[\"test\"]]\n",
    "\n",
    "# 2. Fonksiyonu Çalıştır\n",
    "# 'evaluate_lora_performance' fonksiyonunun daha önceki hücrelerde tanımlı olduğundan emin ol!\n",
    "print(\"\\n--- LoRA Modeli GSM8K-TR Üzerinde Test Ediliyor ---\")\n",
    "results = evaluate_sft_performance(model, tokenizer, test_samples, batch_size=2)\n",
    "\n",
    "# 3. Sonuçları Yazdır\n",
    "print(\"\\n--- ANALİZ SONUÇLARI ---\")\n",
    "for metrik, deger in results.items():\n",
    "    print(f\"{metrik}: {deger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab533253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model ve Tokenizer yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 292/292 [00:00<00:00, 358.84it/s, Materializing param=transformer.wte.weight]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA ağırlıkları (PEFT) giydiriliyor...\n",
      "LoRA modeli başarıyla birleştirildi ve hazır!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "lora_weights_yolu_2 = r\"C:\\Users\\Yusuf\\.cache\\kagglehub\\models\\kubikay\\sft-v2\\pyTorch\\default\\1\\final_model\"\n",
    "# ---------------\n",
    "\n",
    "print(\"Base model ve Tokenizer yükleniyor...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_yolu)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_yolu,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"LoRA ağırlıkları (PEFT) giydiriliyor...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_weights_yolu_2)\n",
    "model = model.to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"LoRA modeli başarıyla birleştirildi ve hazır!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3eb9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K-TR (malhajar) veri seti yükleniyor...\n",
      "Toplam 1318 örnek hazırlandı. (test spliti kullanılıyor)\n",
      "\n",
      "--- GSM8K-TR Üzerinde Değerlendirme Başlatılıyor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Değerlendiriliyor: 100%|██████████| 659/659 [00:59<00:00, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GSM8K-TR EVAL SONUÇLARI ---\n",
      "Top-1 Accuracy (%): 54.83\n",
      "Top-5 Accuracy (%): 74.18\n",
      "Perplexity: 8.2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Veri setini 'main' config'i ile yüklüyoruz\n",
    "print(\"GSM8K-TR (malhajar) veri seti yükleniyor...\")\n",
    "dataset = load_dataset(\"malhajar/gsm8k-tr\", \"main\")\n",
    "\n",
    "# 2. Test verisini hazırla\n",
    "test_samples = []\n",
    "\n",
    "# Bu veri setinde split isimleri genelde 'train' ve 'test'tir.\n",
    "# Kontrol amaçlı split ismini belirliyoruz:\n",
    "split_name = \"test\" if \"test\" in dataset else \"train\"\n",
    "\n",
    "for example in dataset[split_name]:\n",
    "    # Sütun isimleri bu veri setinde 'question' ve 'answer'dır.\n",
    "    full_text = f\"Soru: {example['question']}\\nCevap: {example['answer']}\"\n",
    "    test_samples.append(full_text)\n",
    "\n",
    "print(f\"Toplam {len(test_samples)} örnek hazırlandı. ({split_name} spliti kullanılıyor)\")\n",
    "\n",
    "# 3. Değerlendirmeyi Çalıştır (Önceki hücredeki fonksiyonu kullanır)\n",
    "print(\"\\n--- GSM8K-TR Üzerinde Değerlendirme Başlatılıyor ---\")\n",
    "results = evaluate_model_performance(model, tokenizer, test_samples, batch_size=2)\n",
    "\n",
    "print(\"\\n--- GSM8K-TR EVAL SONUÇLARI ---\")\n",
    "for metrik, deger in results.items():\n",
    "    print(f\"{metrik}: {deger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6748f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
